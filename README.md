# Continual Gradient Low-Rank Projection Fine-Tuning for LLMs
- Official Code for Continual Gradient Low-Rank Projection Fine-Tuning for LLMs
- It is built based on the pretrained T5-large model and llama2 model, and finetuned on our data.

## FrameWork
![image_text](framework/framework.jpg)


## GORP é¡¹ç›®æ–‡ä»¶ç»“æ„è¯¦è§£

è¿™æ˜¯ä¸€ä¸ªåŸºäº**æŒç»­å­¦ä¹ (Continual Learning)**çš„LLMå¾®è°ƒé¡¹ç›®,ä½¿ç”¨**Gradient Low-Rank Projection (GORP)** æ–¹æ³•,ç»“åˆ**LoRA**æŠ€æœ¯è¿›è¡Œå‚æ•°é«˜æ•ˆå¾®è°ƒã€‚

### ğŸ“ **ä¸€ã€é¡¶å±‚æ–‡ä»¶**

- **README.md**: é¡¹ç›®è¯´æ˜æ–‡æ¡£,ä»‹ç»å¦‚ä½•å®‰è£…ã€è®­ç»ƒå’Œè¯„ä¼°æ¨¡å‹
- **requirements.txt**: Pythonä¾èµ–åŒ…åˆ—è¡¨(transformers, deepspeed, peftç­‰)

### ğŸ“ **äºŒã€CL_Benchmark/** - æŒç»­å­¦ä¹ åŸºå‡†æ•°æ®é›†

åŒ…å«å¤šä¸ªNLPä»»åŠ¡çš„æ•°æ®é›†,ç”¨äºæµ‹è¯•æ¨¡å‹çš„æŒç»­å­¦ä¹ èƒ½åŠ›:

- **`BoolQA/`**: å¸ƒå°”é—®ç­”ä»»åŠ¡
- **`COPA/`**: å› æœæ¨ç†ä»»åŠ¡
- **`MultiRC/`**: å¤šé¡¹é€‰æ‹©é˜…è¯»ç†è§£
- **`NLI/`**: è‡ªç„¶è¯­è¨€æ¨ç†(CB, MNLI, RTEå­ä»»åŠ¡)
- **`QQP/`**: é—®é¢˜å¯¹ç›¸ä¼¼åº¦åˆ¤æ–­
- **`SC/`**: æƒ…æ„Ÿåˆ†ç±»(amazon, IMDB, SST-2, yelp)
- **`TC/`**: æ–‡æœ¬åˆ†ç±»(agnews, dbpedia, yahoo)
- **`WiC/`**: è¯ä¹‰æ¶ˆæ­§

### ğŸ“ **ä¸‰ã€configs/** - é…ç½®æ–‡ä»¶ç›®å½•

#### **instruction_config.json**
- æ¯ä¸ªä»»åŠ¡çš„æŒ‡ä»¤æ¨¡æ¿(zero-shot prompts)

#### **`ds_configs/`** - DeepSpeedé…ç½®
- `stage0/1/2/3.config`: ä¸åŒé˜¶æ®µçš„åˆ†å¸ƒå¼è®­ç»ƒé…ç½®
- `eval.config`: è¯„ä¼°é…ç½®
- `stage2_llama.config`: LLaMAæ¨¡å‹ä¸“ç”¨é…ç½®

#### **`order1~6_configs/`** - ä»»åŠ¡é¡ºåºé…ç½®
- å®šä¹‰6ç§ä¸åŒçš„ä»»åŠ¡å­¦ä¹ é¡ºåº
- æ¯ä¸ªæ–‡ä»¶å¤¹åŒ…å«å„ä»»åŠ¡çš„`train/dev/test_tasks.json`
- åœ¨æŒç»­å­¦ä¹ çš„è®¾å®šä¸­ï¼Œæ¨¡å‹éœ€è¦æŒ‰ç…§ç‰¹å®šçš„æ—¶é—´é¡ºåºä¾æ¬¡å­¦ä¹ å¤šä¸ªä¸åŒçš„ä»»åŠ¡ã€‚ç”±äºæ¨¡å‹å®¹æ˜“å‡ºç°â€œç¾éš¾æ€§é—å¿˜â€ï¼ˆå³å­¦ä¼šäº†æ–°ä»»åŠ¡å¿˜äº†æ—§ä»»åŠ¡ï¼‰ï¼Œå­¦ä¹ ä»»åŠ¡çš„å…ˆåé¡ºåºå¾€å¾€ä¼šå¯¹æœ€ç»ˆçš„æ€§èƒ½äº§ç”Ÿå¾ˆå¤§å½±å“ã€‚ä¸åŒçš„ä»»åŠ¡é¡ºåºé…ç½®å±•ç°å‡ºé²æ£’æ€§
---

### ğŸ“ **å››ã€data/** - æ•°æ®å¤„ç†

- **`generate_labels.py`**: ç”Ÿæˆæ ‡ç­¾æ•°æ®çš„è„šæœ¬
- **`gpt2tokenizer/`**: GPT-2åˆ†è¯å™¨é…ç½®æ–‡ä»¶

### ğŸ“ **äº”ã€src/** - æ ¸å¿ƒæºä»£ç  â­â­â­


#### ä¸»è®­ç»ƒæ–‡ä»¶
- **run_uie_lora.py** (628è¡Œ):   UIE (Lu et al., ACL 2022) æ˜¯ä¸€ä¸ªéå¸¸æœ‰åçš„åŸºäº T5 æ¨¡å‹ï¼ˆText-to-Text Transfer Transformerï¼‰çš„æ¡†æ¶ã€‚
	- **ä¸»å…¥å£æ–‡ä»¶**,è´Ÿè´£è®­ç»ƒæµç¨‹æ§åˆ¶
	-  ç¯å¢ƒä¸æ—¥å¿—è®¾ç½® (main å¼€å¤´éƒ¨åˆ†)
		- HfArgumentParser: è§£æå‘½ä»¤è¡Œå‚æ•°æˆ– JSON é…ç½®æ–‡ä»¶ã€‚
		- logging: è®¾ç½®æ—¥å¿—çº§åˆ«ï¼Œç¡®ä¿åœ¨åˆ†å¸ƒå¼è®­ç»ƒä¸­åªæœ‰ä¸»è¿›ç¨‹ï¼ˆRank 0ï¼‰è¾“å‡ºè¯¦ç»†æ—¥å¿—ã€‚
		- set_seed: å›ºå®šéšæœºç§å­ï¼Œä¿è¯å®éªŒå¯å¤ç°ã€‚
	- æ•°æ®é›†åŠ è½½
		- load_dataset(..., "uie_dataset_lora.py"): åŠ è½½è‡ªå®šä¹‰çš„ UIE æ•°æ®é›†ã€‚
		- è¿™æš—ç¤ºä»»åŠ¡æ˜¯å°†â€œå®ä½“æŠ½å–â€ã€â€œå…³ç³»æŠ½å–â€ç­‰ä»»åŠ¡è½¬åŒ–ä¸ºç”Ÿæˆå¼ï¼ˆSeq2Seq æˆ– Causal LMï¼‰çš„å½¢å¼ã€‚
	- æ¨¡å‹ä¸åˆ†è¯å™¨åŠ è½½ (æ ¸å¿ƒé€»è¾‘)
		- è¿™æ®µé€»è¾‘å¤„ç†äº† LLaMA å’Œ Encoder-Decoder (å¦‚ T5) ä¸¤ç±»æ¨¡å‹ï¼š
		- åˆ†è¯å™¨ (Tokenizer)ï¼šé’ˆå¯¹ LLaMA ç‰¹æ®Šå¤„ç†äº† bos, eos, pad token idã€‚
		- æ¨¡å‹æ¶æ„é€‰æ‹©ï¼š
			- LLaMA ä½¿ç”¨ LlamaForCausalLM_with_lossmaskï¼ˆè¿™æ˜¯ä¸€ä¸ªè‡ªå®šä¹‰ç±»ï¼Œé€šå¸¸ç”¨äºåœ¨è®¡ç®— Loss æ—¶å±è”½æ‰ Prompt éƒ¨åˆ†ï¼Œåªè®¡ç®—å›ç­”éƒ¨åˆ†çš„ Lossï¼‰ã€‚
			- å…¶ä»–æ¨¡å‹ä½¿ç”¨ AutoModelForSeq2SeqLMã€‚
		- åŠ è½½ LoRAï¼š
			- æƒ…å†µ1ï¼ˆå¢é‡è®­ç»ƒï¼‰ï¼šå¦‚æœè·¯å¾„ä¸­åŒ…å« adapterï¼Œè¯´æ˜æ˜¯åŠ è½½ä¸€ä¸ªå·²ç»è®­ç»ƒè¿‡çš„ LoRA æƒé‡ (PeftModel.from_pretrained)ã€‚
			- æƒ…å†µ2ï¼ˆä»å¤´å¾®è°ƒï¼‰ï¼šå¦‚æœæ˜¯æ–°è®­ç»ƒï¼Œåˆ™åˆå§‹åŒ–ä¸€ä¸ªæ–°çš„ LoraConfig å¹¶ç”¨ get_peft_model åŒ…è£…æ¨¡å‹ã€‚
	- å‚æ•°å†»ç»“ä¸è§£å†»ç­–ç•¥ (ç²¾ç»†æ§åˆ¶)
		-  å®ƒåŒºåˆ†äº† loranew_ï¼ˆå½“å‰è¦è®­ç»ƒçš„ï¼‰å’Œ lora_ï¼ˆä¹‹å‰è®­ç»ƒå¥½ä½†ç°åœ¨è¦å›ºå®šçš„ï¼‰ã€‚è¿™é€šå¸¸éœ€è¦ä¿®æ”¹ peft æºç æˆ–æ‰‹åŠ¨é‡å‘½åå‚æ•°å±‚æ¥å®ç°ã€‚
	- æ•°æ®æ•´ç†å™¨ (DataCollatorForUIE)
		- è´Ÿè´£å°†æ–‡æœ¬æ•°æ® batch åŒ–ï¼Œå¤„ç† padding (longest)ã€‚
		- å¤„ç† LLaMA å’Œ T5 ä¸åŒçš„ padding é€»è¾‘ã€‚
	- **è‡ªå®šä¹‰ Trainer (UIETrainer)**
		- å®ä¾‹åŒ–äº† UIETrainerã€‚ç»“åˆä¹‹å‰çš„ lamda å‚æ•°ï¼Œè¿™ä¸ª Trainer å†…éƒ¨è‚¯å®šé‡å†™äº† compute_loss æ–¹æ³•ï¼Œ**åˆ©ç”¨ lamda å‚æ•°æ¥å®ç°ä½ æƒ³è¦çš„æ­£åˆ™åŒ–é€»è¾‘**ã€‚
		- å¦‚æœå¼€å¯ denser_evaluationï¼Œä¼šåŠ å…¥é¢å¤–çš„å›è°ƒå‡½æ•°ã€‚
	- è®­ç»ƒåå¤„ç†
		- ä¿å­˜æ¨¡å‹ï¼šåªä¿å­˜ Adapter éƒ¨åˆ† (peft_model_id)ï¼Œä¸ä¿å­˜åº•åº§æ¨¡å‹ï¼ŒèŠ‚çœç©ºé—´ã€‚
		- æ¢¯åº¦çŸ©é˜µåˆ†æï¼šgrad_mat = trainer.get_grad_matrix_from_grad()

| åœºæ™¯ | è¾“å…¥ç¤ºä¾‹ (model_name_or_path) | è§¦å‘é€»è¾‘ | è¡Œä¸ºæ€»ç»“ | æœ€ç»ˆæ¨¡å‹çŠ¶æ€ |
| :--- | :--- | :--- | :--- | :--- |
| **1. åŠ è½½å·²è®­å¥½çš„ Adapter** | `xxx/t5-adapter` <br> `xxx/llama-adapter` | è·¯å¾„å« `"adapter"` | **å…ˆè¯»é…ç½®æ‰¾åº•åº§ï¼Œå†æŒ‚è½½æƒé‡**ã€‚<br>1. è¯»å– Adapter é…ç½®æ–‡ä»¶ï¼Œæ‰¾åˆ°åº•åº§æ¨¡å‹ï¼ˆå¦‚ `t5-large`ï¼‰ã€‚<br>2. åŠ è½½åº•åº§æ¨¡å‹ã€‚ <br>3. ä½¿ç”¨ `PeftModel` å°† Adapter æƒé‡åˆå¹¶è¿›å»ã€‚ | **åŒ…å«æ—§æƒé‡çš„ PEFT æ¨¡å‹**<br>(ç”¨äºå¢é‡è®­ç»ƒæˆ–æ¨ç†) |
| **2. LLaMA ä»å¤´å¾®è°ƒ** | `meta-llama/Llama-2-7b` | è·¯å¾„å« `"llama"` | **ç‰¹åŒ–åŠ è½½ + åˆå§‹åŒ– LoRA**ã€‚<br>1. ä½¿ç”¨ `LlamaTokenizer` å¹¶æ‰‹åŠ¨ä¿®æ­£ç‰¹æ®Š Token IDã€‚<br>2. åŠ è½½ **é­”æ”¹ç‰ˆ** æ¨¡å‹ç±» `LlamaForCausalLM_with_lossmask`ã€‚<br>3. åˆå§‹åŒ–å…¨æ–°çš„ LoRA å±‚ã€‚ | **åˆå§‹åŒ–çš„ PEFT æ¨¡å‹**<br>(Task: Causal LM) |
| **3. T5/BART ä»å¤´å¾®è°ƒ** | `t5-large` <br> `facebook/bart-large` | æ—¢æ—  `"adapter"`<br>ä¹Ÿæ—  `"llama"` | **é€šç”¨åŠ è½½ + åˆå§‹åŒ– LoRA**ã€‚<br>1. ä½¿ç”¨æ ‡å‡†çš„ `AutoTokenizer` å’Œ `AutoModelForSeq2SeqLM`ã€‚<br>2. åˆå§‹åŒ–å…¨æ–°çš„ LoRA å±‚ã€‚ | **åˆå§‹åŒ–çš„ PEFT æ¨¡å‹**<br>(Task: Seq2Seq) |

#### **è®­ç»ƒå™¨**
- **uie_trainer_lora.py** (1297è¡Œ):
  - è‡ªå®šä¹‰Seq2Seqè®­ç»ƒå™¨
  - å®ç°æ¢¯åº¦æ›´æ–°ã€è¯„ä¼°å›è°ƒç­‰
  - **training_stepï¼šé‡Œé¢åŒ…å«äº†è®­ç»ƒè¿‡ç¨‹ï¼ˆå‰å‘ä¼ æ’­+è®¡ç®—æŸå¤±ï¼‰**

#### **æ•°æ®å¤„ç†**
- **uie_dataset_lora.py** (519è¡Œ):
  - æ•°æ®é›†åŠ è½½å’Œé¢„å¤„ç†
  - æ”¯æŒå¤šä»»åŠ¡ã€å¤šæŒ‡ä»¤ç­–ç•¥
- **uie_collator.py** (227è¡Œ):
  - æ•°æ®æ‰¹å¤„ç†collator
  - æ”¯æŒencoder-decoderå’Œdecoder-onlyæ¨¡å‹

#### **è¯„ä¼°**
- **`compute_metrics.py`**: è®¡ç®—è¯„ä¼°æŒ‡æ ‡
- **evaluator.py** (906è¡Œ): è¯¦ç»†çš„è¯„ä¼°å™¨å®ç°(å‡†ç¡®ç‡ã€F1ç­‰)

---

### ğŸ“ **å…­ã€src/Proj_torch/** - æ ¸å¿ƒç®—æ³•å®ç° ğŸ”¥ğŸ”¥ğŸ”¥

**è¿™æ˜¯ä¿®æ”¹ç®—æ³•æ ¸å¿ƒä»£ç çš„åœ°æ–¹!**

#### **proj_projector.py** (105è¡Œ)
- **`ProjProjector`ç±»**: å®ç°æ¢¯åº¦ä½ç§©æŠ•å½±çš„æ ¸å¿ƒç®—æ³•
- **å…³é”®æ–¹æ³•**:
  - `project()`: å°†å…¨ç§©æ¢¯åº¦æŠ•å½±åˆ°ä½ç§©å­ç©ºé—´
  - `project_back()`: å°†ä½ç§©æ¢¯åº¦æŠ•å½±å›åŸç©ºé—´
  - `get_orthogonal_matrix()`: é€šè¿‡SVDåˆ†è§£è·å–æ­£äº¤çŸ©é˜µ
- **æŠ•å½±ç±»å‹**: æ”¯æŒstdã€reverse_stdã€leftã€rightã€fullç­‰å¤šç§æŠ•å½±æ–¹å¼

#### **proj_projector_tensor.py**
- **`ProjProjectorTensor`ç±»**: ç”¨äºå¤„ç†é«˜ç»´å¼ é‡çš„æŠ•å½±(>2ç»´)

#### **adamw.py** (186è¡Œ)
- **`AdamW`ä¼˜åŒ–å™¨**: è‡ªå®šä¹‰çš„AdamWå®ç°
- **æ ¸å¿ƒåŠŸèƒ½**:
  - åœ¨`step()`æ–¹æ³•ä¸­é›†æˆæ¢¯åº¦æŠ•å½±
  - åŠ¨æ€æ›´æ–°æŠ•å½±çŸ©é˜µ(æ¯`update_proj_gap`æ­¥)
  - æ”¯æŒæ¢¯åº¦æŠ•å½±å‰åçš„å¤„ç†

#### **`__init__.py`**
- å¯¼å‡º`ProjAdamW`ä¼˜åŒ–å™¨ä¾›å¤–éƒ¨ä½¿ç”¨


### ğŸ“ **ä¸ƒã€src/peft/** - LoRAå®ç°

ä¿®æ”¹è‡ªHuggingFace PEFTåº“:

- **lora.py** (748è¡Œ):
  - **LoRAå®ç°**: å®šä¹‰LoRAå±‚å’Œé…ç½®ã€é‡Œé¢å°±å®šä¹‰äº†loranewå‚æ•°ã€‘
  - **å…³é”®ä¿®æ”¹**: 
    - `r_sum`: è®°å½•ä¹‹å‰LoRAå‚æ•°ç»´åº¦(ç”¨äºæŒç»­å­¦ä¹ )
    - `save_loranew`: æ˜¯å¦ç‹¬ç«‹ä¿å­˜æ–°LoRAå‚æ•°
- **`peft_model.py`**: PEFTæ¨¡å‹å°è£…
- **`mapping.py`**: æ¨¡å‹æ˜ å°„
- **å…¶ä»–tuners**: adalora, prefix_tuningç­‰

### ğŸ“ **å…«ã€src/model/** - æ¨¡å‹å®šä¹‰
- **`llama.py`**: LLaMAæ¨¡å‹çš„ç‰¹æ®Šå¤„ç†

### ğŸ“ **ä¹ã€src/rouge/** - è¯„ä¼°æŒ‡æ ‡
- ROUGEåˆ†æ•°è®¡ç®—(ç”¨äºæ–‡æœ¬ç”Ÿæˆä»»åŠ¡è¯„ä¼°)


### ğŸ“ åã€scripts/ - è®­ç»ƒè„šæœ¬ï¼Œscripts_llama/ - LLaMAä¸“ç”¨è„šæœ¬
- **run.sh**: ä¸»è¿è¡Œè„šæœ¬,ä¾æ¬¡æ‰§è¡Œorder_1åˆ°order_6
- **`order_1.sh ~ order_6.sh`**: 
  - æ¯ä¸ªè„šæœ¬å®šä¹‰ä¸€ç§ä»»åŠ¡å­¦ä¹ é¡ºåº
  - åŒ…å«å¤šä¸ªä»»åŠ¡çš„è¿ç»­è®­ç»ƒ
  - é…ç½®å­¦ä¹ ç‡ã€æ‰¹å¤§å°ã€LoRAæ¨¡å—ç­‰
- **`run_llama.sh`**: LLaMAæ¨¡å‹çš„è®­ç»ƒæµç¨‹
- **`order_1/2/3.sh`**: LLaMAçš„ä¸‰ç§ä»»åŠ¡é¡ºåº

### ğŸ“ **åäºŒã€logs/ å’Œ logs_llama/** - æ—¥å¿—ç›®å½•
- å­˜å‚¨è®­ç»ƒå’Œæ¨ç†æ—¥å¿—
- `null.txt`: å ä½ç¬¦æ–‡ä»¶


## ğŸ’¡ **ç®—æ³•å·¥ä½œæµç¨‹**

1. **æ•°æ®åŠ è½½**: uie_dataset_lora.py â†’ åŠ è½½ä»»åŠ¡æ•°æ®
2. **æ¨¡å‹åˆå§‹åŒ–**: run_uie_lora.py â†’ åŠ è½½T5/LLaMA + LoRAå±‚
3. **è®­ç»ƒå¾ªç¯**: uie_trainer_lora.py â†’ æ§åˆ¶è®­ç»ƒè¿‡ç¨‹
4. **æ¢¯åº¦è®¡ç®—**: åå‘ä¼ æ’­å¾—åˆ°å…¨ç§©æ¢¯åº¦
5. **æ¢¯åº¦æŠ•å½±**: `ProjProjector.project()` â†’ é™ä½æ¢¯åº¦ç»´åº¦
6. **ä¼˜åŒ–æ›´æ–°**: `AdamW.step()` â†’ åœ¨ä½ç§©ç©ºé—´æ›´æ–°å‚æ•°
7. **æŠ•å½±å›åŸç©ºé—´**: `project_back()` â†’ åº”ç”¨åˆ°åŸæ¨¡å‹å‚æ•°
8. **æŒç»­å­¦ä¹ **: ä¿å­˜æ—§LoRAå‚æ•°,ä¸ºæ–°ä»»åŠ¡æ·»åŠ æ–°LoRAå±‚






## Question
### å‚æ•°å‘½å
ç¬¬ä¸€é˜¶æ®µ LoRA (æ—§å‚æ•°) - æ¥è‡ªä¸Šä¸€ä¸ªä»»åŠ¡
"encoder.block.0.layer.0.SelfAttention.q.lora_A.weight"       # r Ã— in_features
"encoder.block.0.layer.0.SelfAttention.q.lora_B.weight"       # out_features Ã— r

ç¬¬äºŒé˜¶æ®µ LoRA (æ–°å‚æ•°) - å½“å‰ä»»åŠ¡ ã€ä½ æ·»åŠ çš„ã€‘
"encoder.block.0.layer.0.SelfAttention.q.loranew_A.weight"    # r_new Ã— in_features
"encoder.block.0.layer.0.SelfAttention.q.loranew_B.weight"    # out_features Ã— r_new

### ææ¸…æ¥šè®­ç»ƒæ—¶è®­ç»ƒçš„æ˜¯å“ªä¸€ä¸ªå‚æ•°ï¼Ÿ
```
for name, param in model.named_parameters():
    if name.find("loranew_") != -1:
        param.requires_grad = True          # âœ… loranew_A/B å¯ä»¥è®­ç»ƒ
    elif name.find("lora_") != -1:
        param.requires_grad = False         # âŒ lora_A/B è¢«å†»ç»“
    # å…¶ä»–å‚æ•°çš„å¤„ç†...
```
> fix lora_A/B (bases of previous LoRA parameters, loaded in "load_adapter"[peft_momdel.py])
fine-tune loranew_A/B (initialized in "update_layer"[lora.py])
ä¸ºä»€ä¹ˆè¿™æ ·æï¼Ÿå·¥ç¨‹å¯èƒ½åœ¨åšæŒç»­å­¦ä¹ ï¼ˆContinual Learningï¼‰æˆ–å¤šé˜¶æ®µå¾®è°ƒ

#### æƒ…å†µ1ï¸âƒ£ï¼šå½“ model_name_or_path æ˜¯**æœ¬åœ°è·¯å¾„**

```
å‚æ•°: --model_name_or_path initial_model/llama
     |
     v
æ£€æŸ¥ initial_model/llama æ˜¯å¦å­˜åœ¨ï¼Ÿ
     |
     â”œâ”€ YES âœ…
     |   â””â”€ ç›´æ¥ä»æœ¬åœ°åŠ è½½
     |       â”œâ”€ è¯»å– config.json
     |       â”œâ”€ è¯»å– pytorch_model.bin
     |       â”œâ”€ è¯»å– tokenizer.model
     |       â””â”€ è¿”å›æ¨¡å‹å¯¹è±¡
     |
     â””â”€ NO âŒ
         â””â”€ æŠ¥é”™ï¼šFileNotFoundError
             (å› ä¸ºä»£ç è®¤ä¸ºä½ ç»™çš„æ˜¯æœ¬åœ°è·¯å¾„ï¼Œä¸ä¼šå°è¯•ç½‘ç»œä¸‹è½½)
```

**å…³é”®ç‚¹**ï¼šå¦‚æœä½ æä¾›çš„æ˜¯æœ¬åœ°è·¯å¾„ï¼ŒHuggingFace åº“**ä¸ä¼šè‡ªåŠ¨ä»ç½‘ç»œä¸‹è½½**ï¼

#### æƒ…å†µ2ï¸âƒ£ï¼šå½“ model_name_or_path æ˜¯ **HuggingFace model ID**

```
å‚æ•°: --model_name_or_path meta-llama/Llama-2-7b
     |
     v
from_pretrained() çš„åŠ è½½ä¼˜å…ˆçº§:

ä¼˜å…ˆçº§1ï¸âƒ£: æ£€æŸ¥æ˜¯å¦æ˜¯æœ¬åœ°è·¯å¾„ï¼Ÿ
    â””â”€ NO (å› ä¸ºè¿™æ˜¯ model ID æ ¼å¼)

ä¼˜å…ˆçº§2ï¸âƒ£: æ£€æŸ¥ cache_dir ç¼“å­˜ç›®å½•
    é»˜è®¤ç¼“å­˜ç›®å½•: ~/.cache/huggingface/hub/
    â”œâ”€ å­˜åœ¨å®Œæ•´æ¨¡å‹ï¼Ÿ
    |   â”œâ”€ YES âœ… ä»ç¼“å­˜åŠ è½½
    |   â””â”€ NO ç»§ç»­
    â””â”€ å­˜åœ¨ä¸å®Œæ•´ä¸‹è½½ï¼Ÿ
        â””â”€ YES ç»§ç»­æ–­ç‚¹ä¸‹è½½

ä¼˜å…ˆçº§3ï¸âƒ£: ä»ç½‘ç»œä¸‹è½½
    â”œâ”€ è¿æ¥ HuggingFace å®˜æ–¹æœåŠ¡å™¨
    â”œâ”€ ä¸‹è½½æ¨¡å‹æ–‡ä»¶åˆ°ç¼“å­˜ç›®å½•
    â”œâ”€ è§£å‹/éªŒè¯
    â””â”€ åŠ è½½åˆ°å†…å­˜
```
- **ä»£ç ä¸­æ²¡æœ‰è‡ªåŠ¨ä¸‹è½½æ¨¡å‹çš„åŠŸèƒ½** - è„šæœ¬ä¸­çš„ `from_pretrained()` æ–¹æ³•å¯ä»¥ä» HuggingFace è‡ªåŠ¨ä¸‹è½½æ¨¡å‹ï¼Œä½†éœ€è¦ç½‘ç»œè¿æ¥ä¸”é€Ÿåº¦å¯èƒ½è¾ƒæ…¢- **ä»£ç ä¸­æœ‰**ï¼š`src/run_uie_lora.py` ä½¿ç”¨ HuggingFace çš„ `from_pretrained()` æ–¹æ³•
- è¿™ä¸ªæ–¹æ³•å¯ä»¥è‡ªåŠ¨ä» HuggingFace ä¸‹è½½æ¨¡å‹ï¼ˆå¦‚æœä¼ å…¥ model ID è€Œä¸æ˜¯æœ¬åœ°è·¯å¾„ï¼‰
- **åŸä½œè€…ç¡¬ç¼–ç äº†æ¨¡å‹è·¯å¾„** - ä½¿ç”¨çš„æ˜¯ `/data/chenxu/models/t5-large` å’Œ `/data/chenxu/models/llama`ï¼Œè¿™æ˜¯ä»–çš„æœ¬åœ°è·¯å¾„   ä»¥ä¸‹è„šæœ¬æ–‡ä»¶å·²å…¨éƒ¨æ›´æ–°ï¼Œå°†æ¨¡å‹è·¯å¾„ä» `/data/chenxu/models/` æ”¹ä¸º `initial_model/`ï¼š

### å†…å­˜è®¡ç®—/OOMé—®é¢˜è¸©å‘ã€
```
å½“å‰é…ç½®ï¼š
  æ¨¡å‹: LLaMA-7B (çº¦13GB fp32)
  LoRA rank: 8
  è®­ç»ƒæ–¹å¼: DeepSpeed ZeRO Stage 2 + LoRA + GaLore
  Batch size: 1 per device
  Gradient accumulation: 8
  Gradient checkpointing: å¯ç”¨
  ç²¾åº¦: bfloat16
  GPU: 2 Ã— 24GB (å¹¶è¡Œè®­ç»ƒ)
å†…å­˜æ¶ˆè€—åˆ†è§£ï¼š
  æ¨¡å‹æƒé‡ (bfloat16): ~13GB
  LoRA å‚æ•°: å¾ˆå° (rank=8, çº¦100MB)
  ä¼˜åŒ–å™¨çŠ¶æ€ (ZeRO-2åˆ†ç‰‡): ~6-8GB
  æ¢¯åº¦: ~6-7GB (ZeRO-2åˆ†ç‰‡)
  æ¿€æ´»å€¼ (å³ä½¿æœ‰gradient checkpointing): ~2-4GB
  ä¸´æ—¶ç¼“å†²åŒº: ~1-2GB
  å•å¡æ€»éœ€æ±‚: çº¦ 23-24GB
RTX 4090 æ˜¾å­˜: 24GB
```
### try:ç”¨äº†34364MiB=33GB, fp32,æ—¶é—´ï¼š1642-2009ï¼Œçº¦3ä¸ªå°æ—¶(one task)====é‡å¤ä¸å‡ºæ¥


### ä¸å…¼å®¹æŠ¥é”™
```
conda activate aslora && pip install -i https://pypi.tuna.tsinghua.edu.cn/simple 'numpy<2' 'pyarrow==10.0.1' 'datasets==2.13.1' 'fsspec==2023.6.0' 'tqdm==4.65.0'
```
### å°è¯•zero3å¤±è´¥
æ— è®º ZeRO-3 æ˜¯å¦ offloadï¼Œåªè¦å¯ç”¨éƒ½ä¼šå‡ºç°éšè—ç»´åº¦ä¸º 0 çš„é”™è¯¯ã€‚è¿™è¯´æ˜é—®é¢˜æ ¹æœ¬ä¸åœ¨ offloadï¼Œè€Œåœ¨ ZeRO-3 ä¸ gradient_checkpointing çš„æ·±å±‚å…¼å®¹æ€§é—®é¢˜ï¼Œæˆ–è€…ä¸ PEFT/LoRA åœ¨åˆ†ç‰‡ä¸‹çš„å‚æ•°é‡æ„é—®é¢˜ã€‚


### è„šæœ¬é˜…è¯»
```
bash scripts_llama/order_1_optimized.sh outputs_order_1 2 2e-04 ".*mlp.gate_proj.*" "localhost:0,1" 1e-06
#                                         $1           $2  $3       $4                $5              $6


deepspeed --include $5 --master_port $port src/run_uie_lora.py \
   --do_train \
   --do_predict \
   --predict_with_generate \
   --model_name_or_path initial_model/llama \
   --data_dir CL_Benchmark \
   --task_config_dir configs/order1_configs/dbpedia \
   --instruction_file configs/instruction_config.json \
   --instruction_strategy single \
   --output_dir logs_and_outputs_llama/order_1/$1/1-dbpedia \
   --per_device_train_batch_size 1 \
   --per_device_eval_batch_size 4 \
   --gradient_accumulation_steps 8 \
   --learning_rate $3 \
   --num_train_epochs 1 \
   --deepspeed configs/ds_configs/stage2_llama.config \
   --run_name order1_round1 \
   --max_source_length 512 \
   --max_target_length 50 \
   --generation_max_length 50 \
   --add_task_name True \
   --add_dataset_name True \
   --overwrite_output_dir \
   --overwrite_cache \
   --lr_scheduler_type constant \
   --warmup_steps 0 \
   --logging_strategy steps \
   --logging_steps 10 \
   --evaluation_strategy no \
   --save_strategy no \
   --save_steps 1500 \
   --lamda_1 0.5 \
   --lamda_2 0 \
   --lora_modules ".*self_attn.(q_proj|v_proj).*" \
   --optim_target_modules $4 \  //è¯´æ˜Galoreè¢«å¯ç”¨äº†
   --proj_lora_modules ".*self_attn.(q_proj|v_proj).loranew_A.*" \
   --galore_rank $2 \  
   --galore_scale 0.25 \
   --galore_lr $6 \
   --gradient_checkpointing True
```
æ‰“å°å‡ºæ¥`trainable params: 1447034880 || all params: 6742609920 || trainable%: 21.461049907511185`ï¼Œè¯´æ˜galoreå…¨å‚è®­ç»ƒäº†

`12/15/2025 13:37:48 - WARNING - __main__ - Process rank: 1, device: cuda:1, n_gpu: 1distributed training: True, 16-bits training: False` è¯´æ˜ä½¿ç”¨äº†32ä½
```
if 'adapter' in model_args.model_name_or_path: # add lora-adapter to the original model
        model = model_class.from_pretrained(
            config.base_model_name_or_path,
            torch_dtype=torch.float16 if training_args.fp16 else torch.float32  #æ–°åŠ ï¼Œæƒé‡çš„åŠ è½½æ–¹å¼
        )  #åŠ è½½ åº•åº§æ¨¡å‹ çš„æƒé‡ 
```
åœ¨ä»£ç ä¸­åšå‡ºå¦‚ä¸‹ä¿®æ­£ï¼Œè§£å†³ä¸çŸ¥é“å¯¼å…¥ç²¾åº¦ä¸ºå¤šå°‘çš„é—®é¢˜

### GORPç”¨ä¸äº†BF16
```
RuntimeError: cusolver error: CUSOLVER_STATUS_EXECUTION_FAILED
File ".../Proj_torch/proj_projector.py", line 83, in get_orthogonal_matrix
    U, s, Vh = torch.linalg.svd(matrix, full_matrices = False)
```
åŸå› ï¼šSVD (å¥‡å¼‚å€¼åˆ†è§£) æ˜¯ä¸€ä¸ªæ•°å€¼è®¡ç®—å¯†é›†çš„æ“ä½œï¼Œå¯¹æ•°å€¼ç²¾åº¦è¦æ±‚æé«˜ã€‚å½“ç”¨ float16 è¿›è¡Œ SVD æ—¶ï¼Œä¼šå‡ºç°æ•°å€¼ä¸ç¨³å®šçš„é—®é¢˜ã€‚


### æŠ¥é”™ï¼šé…ç½®ç²¾åº¦å‡ºç°å†²çª
```
ValueError: Please correct the following DeepSpeed config values that mismatch TrainingArguments values:
- ds fp16.enabled=True vs hf fp16|fp16_full_eval+fp16_backend(amp)=False
```
DeepSpeed é…ç½®æ–‡ä»¶ ([stage2_llama.config](configs/ds_configs/stage2_llama.config)stage2_llama.config)ï¼š
è®¾ç½®äº† fp16.enabled=Trueï¼ˆå¯ç”¨åŠç²¾åº¦æµ®ç‚¹ï¼‰
HuggingFace è®­ç»ƒå‚æ•°ï¼š
æ²¡æœ‰å¯ç”¨ fp16ï¼ˆé»˜è®¤ä½¿ç”¨ float32ï¼‰
å³ä½ çš„è„šæœ¬ä¸­æ²¡æœ‰ --fp16 å‚æ•°
åŸºåº§æ¨¡å‹åŠ è½½æ—¶åªçœ‹å‘½ä»¤è¡Œå‚æ•°ï¼ˆ--bf16/--fp16ï¼‰ï¼Œä¸çœ‹ DeepSpeed é…ç½®æ–‡ä»¶ã€‚
é˜¶æ®µ2ï¼šDeepSpeed åˆå§‹åŒ–ï¼ˆTrainer åˆ›å»ºæ—¶ï¼‰

DeepSpeed è¯»å–é…ç½®æ–‡ä»¶ï¼ˆå¦‚ stage2_llama.configï¼‰
å¦‚æœé…ç½®ä¸æ¨¡å‹åŠ è½½ç²¾åº¦ä¸ä¸€è‡´ï¼ŒDeepSpeed ä¼šå°è¯•è½¬æ¢
ä½†è½¬æ¢æœ‰é¢å¤–å¼€é”€å’Œé£é™©
ä½ çš„ DeepSpeed é…ç½®ï¼š
{
    "bfloat16": {"enabled": true},
    "fp16": {"enabled": "auto"}
}
bf16: true â†’ DeepSpeed æœŸæœ› BF16
fp16: auto â†’ æ ¹æ®å‘½ä»¤è¡Œè‡ªåŠ¨å†³å®š

### ä»€ä¹ˆæ˜¯CPU offload

æ ‡å‡†æ–¹å¼ï¼š
æ¨¡å‹å‚æ•° â†’ GPU â†’ è®¡ç®— â†’ åå‘ä¼ æ’­

CPU Offloadï¼š
æ¨¡å‹å‚æ•° â†’ CPU (å­˜å‚¨) 
è®¡ç®—æ—¶ â†’ æŠŠå‚æ•°ç§»åˆ°GPU â†’ è®¡ç®— â†’ ç§»å›CPU
æƒ…å†µ1ï¼šä½¿ç”¨äº† Stage 2 + CPU Offloadï¼ˆå¯èƒ½ï¼‰
ä»ä½ çš„è„šæœ¬ï¼š
```
{
  "zero_optimization": {
    "stage": 2,
    "offload_optimizer": {
      "device": "cpu",
      "pin_memory": true
    }
  }
}
```


dbpedia 43151MiB
amazon 17562MiB



- try: order_1_optimized.sh  ä¸èƒ½è·‘ï¼š
```
============================================================
12/15/2025 15:50:55 - WARNING - __main__ - æ¨¡å‹å‚æ•°ç²¾åº¦éªŒè¯:
12/15/2025 15:50:55 - WARNING - __main__ - ============================================================
12/15/2025 15:50:55 - WARNING - __main__ -   float16: 291 ä¸ªå‚æ•°
12/15/2025 15:50:55 - WARNING - __main__ -     ç¤ºä¾‹å‚æ•°: ['base_model.model.model.embed_tokens.weight', 'base_model.model.model.layers.0.self_attn.q_proj.weight', 'base_model.model.model.layers.0.self_attn.k_proj.weight']
12/15/2025 15:50:55 - WARNING - __main__ -   float32: 256 ä¸ªå‚æ•°
12/15/2025 15:50:55 - WARNING - __main__ -     ç¤ºä¾‹å‚æ•°: ['base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.0.self_attn.q_proj.loranew_A.default.weight']
12/15/2025 15:50:55 - WARNING - __main__ - ============================================================

12/15/2025 15:50:55 - WARNING - __main__ - âœ“ ç¬¬ä¸€ä¸ªå‚æ•°ç²¾åº¦: torch.float16
12/15/2025 15:50:55 - WARNING - __main__ - âœ“ ç¬¬ä¸€ä¸ªå‚æ•°è®¾å¤‡: cpu

-----Gradient checkpointing: True -----
trainable params: 1447034880 || all params: 6742609920 || trainable%: 21.461049907511185
```
- tryï¼šèƒ½è·‘ï¼Ÿç¬¬10ä¸ªæ ·ä¾‹å¡ä½
```
-----Gradient checkpointing: True -----
12/15/2025 16:21:18 - WARNING - __main__ - 
============================================================
12/15/2025 16:21:18 - WARNING - __main__ - æ¨¡å‹å‚æ•°ç²¾åº¦éªŒè¯:
12/15/2025 16:21:18 - WARNING - __main__ - ============================================================
12/15/2025 16:21:18 - WARNING - __main__ -   float16: 291 ä¸ªå‚æ•°
12/15/2025 16:21:18 - WARNING - __main__ -     ç¤ºä¾‹å‚æ•°: ['base_model.model.model.embed_tokens.weight', 'base_model.model.model.layers.0.self_attn.q_proj.weight', 'base_model.model.model.layers.0.self_attn.k_proj.weight']
12/15/2025 16:21:18 - WARNING - __main__ -   float32: 256 ä¸ªå‚æ•°
12/15/2025 16:21:18 - WARNING - __main__ -     ç¤ºä¾‹å‚æ•°: ['base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.0.self_attn.q_proj.loranew_A.default.weight']
12/15/2025 16:21:18 - WARNING - __main__ - ============================================================

12/15/2025 16:21:18 - WARNING - __main__ - âœ“ ç¬¬ä¸€ä¸ªå‚æ•°ç²¾åº¦: torch.float16
12/15/2025 16:21:18 - WARNING - __main__ - âœ“ ç¬¬ä¸€ä¸ªå‚æ•°è®¾å¤‡: cpu
```


### è¿‡ç¨‹ä¸­å‡ºç°è„šæœ¬ä¸€æ ·ä½†ä¸€ä¸ªOOMå¦ä¸€ä¸ªæ²¡æœ‰çš„é—®é¢˜ï¼Ÿ  **æœªè§£å†³**
ä¸ºä»€ä¹ˆä¸ä¸€æ ·ï¼Ÿorder_1æœ‰æ£€æŸ¥ç‚¹è€Œorder_1_optimized_lowmem_testæ²¡æœ‰
```
resume_from_checkpoint (`str` or `bool`, *optional*):
    If a `str`, local path to a saved checkpoint as saved by a previous instance of [`Trainer`]. If a
    `bool` and equals `True`, load the last checkpoint in *args.output_dir* as saved by a previous instance
    of [`Trainer`]. If present, training will resume from the model/optimizer/scheduler states loaded here.
```
ä½†æ ¹æ®ä»£ç é€»è¾‘ line 304ï¼Œå¦‚æœ output_dir å·²å­˜åœ¨ä¸”æœ‰ checkpointï¼Œä¼šè‡ªåŠ¨ resumeã€‚ä¸ºäº†å®Œå…¨é¿å…ä»»ä½• resume è¡Œä¸ºï¼Œå»ºè®®æ˜¾å¼æ·»åŠ ï¼š
è¿™æ ·å¯ä»¥æ˜ç¡®ç¦æ­¢ä»»ä½• checkpoint åŠ è½½ã€‚




### deepspeed_init
1. deepspeed_init ä¼šä¸ä¼šè‡ªåŠ¨æŠŠ self.model å˜æˆ Engineï¼Ÿ
ç­”æ¡ˆï¼šæ˜¯çš„ï¼Œå®ƒåº”è¯¥ä¼šï¼ˆæœ‰å‰¯ä½œç”¨ï¼‰ã€‚
deepspeed_init æ˜¯ Hugging Face Trainer å†…éƒ¨çš„ä¸€ä¸ªè¾…åŠ©å‡½æ•°ã€‚å®ƒçš„ä½œç”¨æµç¨‹é€šå¸¸å¦‚ä¸‹ï¼š
å®ƒè¯»å– training_args.deepspeed é…ç½®æ–‡ä»¶ã€‚
å®ƒè°ƒç”¨åº•å±‚çš„ deepspeed.initialize(model=self.model, ...)ã€‚
å…³é”®ç‚¹ï¼šå®ƒä¼šæŠŠåˆå§‹åŒ–å¥½çš„ DeepSpeed Engine èµ‹å€¼å› self.modelï¼ŒåŒæ—¶ä¹Ÿä¼šæ›´æ–° self.optimizer å’Œ self.lr_schedulerã€‚
å®ƒé€šå¸¸è¿˜ä¼šæŠŠ self.deepspeed è¿™ä¸ªå±æ€§æŒ‡å‘è¿™ä¸ª Engineã€‚
ä½†æ˜¯ï¼Œåœ¨ä½ ä¹‹å‰çš„æŠ¥é”™ä¸­ï¼š
AttributeError: 'LlamaForCausalLM_with_lossmask' object has no attribute 'backward'
è¿™è¯´æ˜è™½ç„¶ deepspeed_init è¿è¡Œäº†ï¼Œä½†åœ¨é‚£ä¸ªç‰¹å®šçš„æ—¶é—´ç‚¹ï¼Œä»£ç å¼•ç”¨çš„å¯¹è±¡ï¼ˆself.deepspeedï¼‰å¹¶æ²¡æœ‰æˆåŠŸæŒ‡å‘ Engineï¼Œæˆ–è€… self.model è¢«æŸäº›åç»­æ“ä½œï¼ˆæ¯”å¦‚ _wrap_modelï¼‰åˆå˜å›äº†åŸå§‹æ¨¡å‹ã€‚
2. DeepSpeed æ˜¯ Accelerator å—ï¼Ÿ
ä¸å®Œå…¨æ˜¯ï¼Œå®ƒä»¬æ˜¯ä¸¤å±‚å…³ç³»ã€‚
DeepSpeed: æ˜¯ä¸€ä¸ªä¼˜åŒ–å¼•æ“ï¼ˆEngineï¼‰ã€‚å®ƒç›´æ¥æ¥ç®¡æ¨¡å‹çš„è®­ç»ƒã€æ¢¯åº¦æ›´æ–°ã€æ˜¾å­˜ç®¡ç†ï¼ˆZeROï¼‰ã€‚å®ƒæœ‰ç‚¹åƒæ˜¯ä¸€ä¸ªâ€œè¶…çº§ä¼˜åŒ–ç‰ˆçš„ PyTorchâ€ã€‚
Accelerator (Hugging Face accelerate åº“): æ˜¯ä¸€ä¸ª**â€œç»Ÿç­¹è€…â€æˆ–â€œåŒ…è£…å™¨â€**ã€‚å®ƒçš„ä½œç”¨æ˜¯è®©åŒä¸€å¥—ä»£ç å¯ä»¥è¿è¡Œåœ¨ GPUã€TPUã€å¤šå¡ã€DeepSpeed ç­‰ä¸åŒç¯å¢ƒä¸Šã€‚
åœ¨ä½ çš„ä»£ç ä¸­ï¼š
self.accelerator æ˜¯ accelerate åº“çš„å¯¹è±¡ã€‚å½“ä½ åœ¨å‚æ•°é‡Œå¼€å¯ DeepSpeed æ—¶ï¼Œaccelerator å†…éƒ¨ ä¼šé›†æˆ DeepSpeedã€‚
é€šå¸¸çš„å†™æ³•æœ‰ä¸¤ç§ï¼š
æ—§å†™æ³•ï¼ˆä½ çš„ä»£ç åå‘è¿™ç§ï¼‰ï¼šæ‰‹åŠ¨è°ƒç”¨ deepspeed_initï¼Œæ˜¾å¼ç®¡ç† DeepSpeedã€‚
æ–°å†™æ³•ï¼ˆæ¨èï¼‰ï¼šå®Œå…¨ä¸è°ƒ deepspeed_initï¼Œç›´æ¥ç”¨ model, optim = accelerator.prepare(model, optim)ã€‚accelerator ä¼šè‡ªåŠ¨å‘ç°ä½ è¦ç”¨ DeepSpeed å¹¶å¸®ä½ åˆå§‹åŒ–å¥½ä¸€åˆ‡ã€‚

### try:ç”¨äº†26736MiBï¼Œ fp32, åˆ é™¤Galoreå¾®è°ƒï¼ˆå¦åˆ™OOMï¼‰   å¤±è´¥ï¼Œå‡ºç°åˆ†å¸ƒå¼é€šä¿¡çš„é—®é¢˜

åœ¨ Hugging Face çš„ TrainingArguments ä¸­å°† fp16 è®¾ç½®ä¸º Trueï¼Œå¼€å¯çš„å°±æ˜¯â€œæ··åˆç²¾åº¦è®­ç»ƒâ€ï¼ˆMixed Precision Trainingï¼‰ï¼Œè€Œä¸æ˜¯â€œçº¯FP16è®­ç»ƒâ€ã€‚
å®ƒçš„å†…éƒ¨å·¥ä½œæœºåˆ¶æ­£å¦‚ä½ æ‰€æè¿°çš„ï¼šâ€œè®¡ç®—ç”¨ 16ä½ï¼Œæ›´æ–°ç”¨ 32ä½â€ã€‚





[rank1]: ValueError: Argument `synced_gpus` is not a valid argument of `GenerationConfig`. It should be passed to `generate()` (or a pipeline) directly.
[rank0]: Traceback (most recent call last):
[rank0]:   File "/home/dengkn/N-LoRA/src/run_N_lora.py", line 620, in <module>
[rank0]:     main()
[rank0]:   File "/home/dengkn/N-LoRA/src/run_N_lora.py", line 597, in main
[rank0]:     predict_results = trainer.predict(
[rank0]:   File "/home/dengkn/miniforge3/envs/aslora/lib/python3.9/site-packages/transformers/trainer_seq2seq.py", line 244, in predict
[rank0]:     return super().predict(test_dataset, ignore_keys=ignore_keys, metric_key_prefix=metric_key_prefix)
[rank0]:   File "/home/dengkn/miniforge3/envs/aslora/lib/python3.9/site-packages/transformers/trainer.py", line 3754, in predict
[rank0]:     output = eval_loop(
[rank0]:   File "/home/dengkn/N-LoRA/src/uie_trainer_lora.py", line 203, in evaluation_loop
[rank0]:     loss, logits, labels = self.prediction_step(model, inputs, prediction_loss_only, ignore_keys=ignore_keys)
[rank0]:   File "/home/dengkn/N-LoRA/src/uie_trainer_lora.py", line 337, in prediction_step
[rank0]:     generation_config = GenerationConfig(**gen_kwargs)
[rank0]:   File "/home/dengkn/miniforge3/envs/aslora/lib/python3.9/site-packages/transformers/generation/configuration_utils.py", line 453, in __init__
[rank0]:     self.validate(is_init=True)
[rank0]:   File "/home/dengkn/miniforge3/envs/aslora/lib/python3.9/site-packages/transformers/generation/configuration_utils.py", line 725, in validate
[rank0]:     raise ValueError(
[rank0]: ValueError: Argument `synced_gpus` is not a valid argument of `GenerationConfig`. It should be passed to `generate()` (or a pipeline) directly.

## Setup

You can install the required libraries by running 

```
pip install -r requirements.txt
```

You are also required to download the t5-large model from huggingface, put it to the folder named ```initial_model```, and rename the model folder as 't5-large'.

LLaMA2 HF is also supported. You can put your llama2 hf model to the folder named ```initial_model``` and rename the model folder as 'llama'.


## Training and Evaluation

For t5-large:

You can reproduce our experiments of order 1 to 6 by simply running ```scripts/run.sh```.

The model you have trained will be saved in ```logs_and_outputs/order_(1 to 6)/outputs_order_(1 to 6)```.

The result of each task will be saved in ```logs_and_outputs/order_(1 to 6)/outputs/TASK_NAME/predict_results.json```.

You can also check the logs during training and infering in  ```logs/order_(1 to 6).log```

For LLaMA2:

You can reproduce our experiments of order 1 to 3 by simply running ```scripts_llama/run_llama.sh```.

The model you have trained will be saved in ```logs_and_outputs_llama/order_1(2 or 3)/outputs```.

The result of each task will be saved in ```logs_and_outputs_llama/order_1(2 or 3)/outputs/TASK_NAME/predict_results.json```.

You can also check the logs during training and infering in  ```logs_llama/order_1(2 or 3)/order_1(2 or 3).log```


## Citation
```markdown
@inproceedings{wang-etal-2025-continual,
    title = "Continual Gradient Low-Rank Projection Fine-Tuning for {LLM}s",
    author = "Wang, Chenxu  and
      Lyu, Yilin  and
      Sun, Zicheng  and
      Jing, Liping",
    booktitle = "Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    year = "2025",
    publisher = "Association for Computational Linguistics",
    pages = "14815--14829",
}
```
## Acknowledgment
We acknowledge the publicly available codebase of [O-LoRA](https://github.com/cmnfriend/O-LoRA)
