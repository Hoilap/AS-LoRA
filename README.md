# Continual Gradient Low-Rank Projection Fine-Tuning for LLMs
- Official Code for Continual Gradient Low-Rank Projection Fine-Tuning for LLMs
- It is built based on the pretrained T5-large model and llama2 model, and finetuned on our data.

## FrameWork
![image_text](framework/framework.jpg)


## GORP é¡¹ç›®æ–‡ä»¶ç»“æ„è¯¦è§£

è¿™æ˜¯ä¸€ä¸ªåŸºäº**æŒç»­å­¦ä¹ (Continual Learning)**çš„LLMå¾®è°ƒé¡¹ç›®,ä½¿ç”¨**Gradient Low-Rank Projection (GORP)** æ–¹æ³•,ç»“åˆ**LoRA**æŠ€æœ¯è¿›è¡Œå‚æ•°é«˜æ•ˆå¾®è°ƒã€‚

### ğŸ“ **ä¸€ã€é¡¶å±‚æ–‡ä»¶**

- **README.md**: é¡¹ç›®è¯´æ˜æ–‡æ¡£,ä»‹ç»å¦‚ä½•å®‰è£…ã€è®­ç»ƒå’Œè¯„ä¼°æ¨¡å‹
- **requirements.txt**: Pythonä¾èµ–åŒ…åˆ—è¡¨(transformers, deepspeed, peftç­‰)

### ğŸ“ **äºŒã€CL_Benchmark/** - æŒç»­å­¦ä¹ åŸºå‡†æ•°æ®é›†

åŒ…å«å¤šä¸ªNLPä»»åŠ¡çš„æ•°æ®é›†,ç”¨äºæµ‹è¯•æ¨¡å‹çš„æŒç»­å­¦ä¹ èƒ½åŠ›:

- **`BoolQA/`**: å¸ƒå°”é—®ç­”ä»»åŠ¡
- **`COPA/`**: å› æœæ¨ç†ä»»åŠ¡
- **`MultiRC/`**: å¤šé¡¹é€‰æ‹©é˜…è¯»ç†è§£
- **`NLI/`**: è‡ªç„¶è¯­è¨€æ¨ç†(CB, MNLI, RTEå­ä»»åŠ¡)
- **`QQP/`**: é—®é¢˜å¯¹ç›¸ä¼¼åº¦åˆ¤æ–­
- **`SC/`**: æƒ…æ„Ÿåˆ†ç±»(amazon, IMDB, SST-2, yelp)
- **`TC/`**: æ–‡æœ¬åˆ†ç±»(agnews, dbpedia, yahoo)
- **`WiC/`**: è¯ä¹‰æ¶ˆæ­§

### ğŸ“ **ä¸‰ã€configs/** - é…ç½®æ–‡ä»¶ç›®å½•

#### **instruction_config.json**
- æ¯ä¸ªä»»åŠ¡çš„æŒ‡ä»¤æ¨¡æ¿(zero-shot prompts)

#### **`ds_configs/`** - DeepSpeedé…ç½®
- `stage0/1/2/3.config`: ä¸åŒé˜¶æ®µçš„åˆ†å¸ƒå¼è®­ç»ƒé…ç½®
- `eval.config`: è¯„ä¼°é…ç½®
- `stage2_llama.config`: LLaMAæ¨¡å‹ä¸“ç”¨é…ç½®

#### **`order1~6_configs/`** - ä»»åŠ¡é¡ºåºé…ç½®
- å®šä¹‰6ç§ä¸åŒçš„ä»»åŠ¡å­¦ä¹ é¡ºåº
- æ¯ä¸ªæ–‡ä»¶å¤¹åŒ…å«å„ä»»åŠ¡çš„`train/dev/test_tasks.json`
- åœ¨æŒç»­å­¦ä¹ çš„è®¾å®šä¸­ï¼Œæ¨¡å‹éœ€è¦æŒ‰ç…§ç‰¹å®šçš„æ—¶é—´é¡ºåºä¾æ¬¡å­¦ä¹ å¤šä¸ªä¸åŒçš„ä»»åŠ¡ã€‚ç”±äºæ¨¡å‹å®¹æ˜“å‡ºç°â€œç¾éš¾æ€§é—å¿˜â€ï¼ˆå³å­¦ä¼šäº†æ–°ä»»åŠ¡å¿˜äº†æ—§ä»»åŠ¡ï¼‰ï¼Œå­¦ä¹ ä»»åŠ¡çš„å…ˆåé¡ºåºå¾€å¾€ä¼šå¯¹æœ€ç»ˆçš„æ€§èƒ½äº§ç”Ÿå¾ˆå¤§å½±å“ã€‚ä¸åŒçš„ä»»åŠ¡é¡ºåºé…ç½®å±•ç°å‡ºé²æ£’æ€§
---

### ğŸ“ **å››ã€data/** - æ•°æ®å¤„ç†

- **`generate_labels.py`**: ç”Ÿæˆæ ‡ç­¾æ•°æ®çš„è„šæœ¬
- **`gpt2tokenizer/`**: GPT-2åˆ†è¯å™¨é…ç½®æ–‡ä»¶

### ğŸ“ **äº”ã€src/** - æ ¸å¿ƒæºä»£ç  â­â­â­


#### ä¸»è®­ç»ƒæ–‡ä»¶
- **run_uie_lora.py** (628è¡Œ):   UIE (Lu et al., ACL 2022) æ˜¯ä¸€ä¸ªéå¸¸æœ‰åçš„åŸºäº T5 æ¨¡å‹ï¼ˆText-to-Text Transfer Transformerï¼‰çš„æ¡†æ¶ã€‚
	- **ä¸»å…¥å£æ–‡ä»¶**,è´Ÿè´£è®­ç»ƒæµç¨‹æ§åˆ¶
	-  ç¯å¢ƒä¸æ—¥å¿—è®¾ç½® (main å¼€å¤´éƒ¨åˆ†)
		- HfArgumentParser: è§£æå‘½ä»¤è¡Œå‚æ•°æˆ– JSON é…ç½®æ–‡ä»¶ã€‚
		- logging: è®¾ç½®æ—¥å¿—çº§åˆ«ï¼Œç¡®ä¿åœ¨åˆ†å¸ƒå¼è®­ç»ƒä¸­åªæœ‰ä¸»è¿›ç¨‹ï¼ˆRank 0ï¼‰è¾“å‡ºè¯¦ç»†æ—¥å¿—ã€‚
		- set_seed: å›ºå®šéšæœºç§å­ï¼Œä¿è¯å®éªŒå¯å¤ç°ã€‚
	- æ•°æ®é›†åŠ è½½
		- load_dataset(..., "uie_dataset_lora.py"): åŠ è½½è‡ªå®šä¹‰çš„ UIE æ•°æ®é›†ã€‚
		- è¿™æš—ç¤ºä»»åŠ¡æ˜¯å°†â€œå®ä½“æŠ½å–â€ã€â€œå…³ç³»æŠ½å–â€ç­‰ä»»åŠ¡è½¬åŒ–ä¸ºç”Ÿæˆå¼ï¼ˆSeq2Seq æˆ– Causal LMï¼‰çš„å½¢å¼ã€‚
	- æ¨¡å‹ä¸åˆ†è¯å™¨åŠ è½½ (æ ¸å¿ƒé€»è¾‘)
		- è¿™æ®µé€»è¾‘å¤„ç†äº† LLaMA å’Œ Encoder-Decoder (å¦‚ T5) ä¸¤ç±»æ¨¡å‹ï¼š
		- åˆ†è¯å™¨ (Tokenizer)ï¼šé’ˆå¯¹ LLaMA ç‰¹æ®Šå¤„ç†äº† bos, eos, pad token idã€‚
		- æ¨¡å‹æ¶æ„é€‰æ‹©ï¼š
			- LLaMA ä½¿ç”¨ LlamaForCausalLM_with_lossmaskï¼ˆè¿™æ˜¯ä¸€ä¸ªè‡ªå®šä¹‰ç±»ï¼Œé€šå¸¸ç”¨äºåœ¨è®¡ç®— Loss æ—¶å±è”½æ‰ Prompt éƒ¨åˆ†ï¼Œåªè®¡ç®—å›ç­”éƒ¨åˆ†çš„ Lossï¼‰ã€‚
			- å…¶ä»–æ¨¡å‹ä½¿ç”¨ AutoModelForSeq2SeqLMã€‚
		- åŠ è½½ LoRAï¼š
			- æƒ…å†µ1ï¼ˆå¢é‡è®­ç»ƒï¼‰ï¼šå¦‚æœè·¯å¾„ä¸­åŒ…å« adapterï¼Œè¯´æ˜æ˜¯åŠ è½½ä¸€ä¸ªå·²ç»è®­ç»ƒè¿‡çš„ LoRA æƒé‡ (PeftModel.from_pretrained)ã€‚
			- æƒ…å†µ2ï¼ˆä»å¤´å¾®è°ƒï¼‰ï¼šå¦‚æœæ˜¯æ–°è®­ç»ƒï¼Œåˆ™åˆå§‹åŒ–ä¸€ä¸ªæ–°çš„ LoraConfig å¹¶ç”¨ get_peft_model åŒ…è£…æ¨¡å‹ã€‚
	- å‚æ•°å†»ç»“ä¸è§£å†»ç­–ç•¥ (ç²¾ç»†æ§åˆ¶)
		-  å®ƒåŒºåˆ†äº† loranew_ï¼ˆå½“å‰è¦è®­ç»ƒçš„ï¼‰å’Œ lora_ï¼ˆä¹‹å‰è®­ç»ƒå¥½ä½†ç°åœ¨è¦å›ºå®šçš„ï¼‰ã€‚è¿™é€šå¸¸éœ€è¦ä¿®æ”¹ peft æºç æˆ–æ‰‹åŠ¨é‡å‘½åå‚æ•°å±‚æ¥å®ç°ã€‚
	- æ•°æ®æ•´ç†å™¨ (DataCollatorForUIE)
		- è´Ÿè´£å°†æ–‡æœ¬æ•°æ® batch åŒ–ï¼Œå¤„ç† padding (longest)ã€‚
		- å¤„ç† LLaMA å’Œ T5 ä¸åŒçš„ padding é€»è¾‘ã€‚
	- **è‡ªå®šä¹‰ Trainer (UIETrainer)**
		- å®ä¾‹åŒ–äº† UIETrainerã€‚ç»“åˆä¹‹å‰çš„ lamda å‚æ•°ï¼Œè¿™ä¸ª Trainer å†…éƒ¨è‚¯å®šé‡å†™äº† compute_loss æ–¹æ³•ï¼Œ**åˆ©ç”¨ lamda å‚æ•°æ¥å®ç°ä½ æƒ³è¦çš„æ­£åˆ™åŒ–é€»è¾‘**ã€‚
		- å¦‚æœå¼€å¯ denser_evaluationï¼Œä¼šåŠ å…¥é¢å¤–çš„å›è°ƒå‡½æ•°ã€‚
	- è®­ç»ƒåå¤„ç†
		- ä¿å­˜æ¨¡å‹ï¼šåªä¿å­˜ Adapter éƒ¨åˆ† (peft_model_id)ï¼Œä¸ä¿å­˜åº•åº§æ¨¡å‹ï¼ŒèŠ‚çœç©ºé—´ã€‚
		- æ¢¯åº¦çŸ©é˜µåˆ†æï¼šgrad_mat = trainer.get_grad_matrix_from_grad()

| åœºæ™¯ | è¾“å…¥ç¤ºä¾‹ (model_name_or_path) | è§¦å‘é€»è¾‘ | è¡Œä¸ºæ€»ç»“ | æœ€ç»ˆæ¨¡å‹çŠ¶æ€ |
| :--- | :--- | :--- | :--- | :--- |
| **1. åŠ è½½å·²è®­å¥½çš„ Adapter** | `xxx/t5-adapter` <br> `xxx/llama-adapter` | è·¯å¾„å« `"adapter"` | **å…ˆè¯»é…ç½®æ‰¾åº•åº§ï¼Œå†æŒ‚è½½æƒé‡**ã€‚<br>1. è¯»å– Adapter é…ç½®æ–‡ä»¶ï¼Œæ‰¾åˆ°åº•åº§æ¨¡å‹ï¼ˆå¦‚ `t5-large`ï¼‰ã€‚<br>2. åŠ è½½åº•åº§æ¨¡å‹ã€‚ <br>3. ä½¿ç”¨ `PeftModel` å°† Adapter æƒé‡åˆå¹¶è¿›å»ã€‚ | **åŒ…å«æ—§æƒé‡çš„ PEFT æ¨¡å‹**<br>(ç”¨äºå¢é‡è®­ç»ƒæˆ–æ¨ç†) |
| **2. LLaMA ä»å¤´å¾®è°ƒ** | `meta-llama/Llama-2-7b` | è·¯å¾„å« `"llama"` | **ç‰¹åŒ–åŠ è½½ + åˆå§‹åŒ– LoRA**ã€‚<br>1. ä½¿ç”¨ `LlamaTokenizer` å¹¶æ‰‹åŠ¨ä¿®æ­£ç‰¹æ®Š Token IDã€‚<br>2. åŠ è½½ **é­”æ”¹ç‰ˆ** æ¨¡å‹ç±» `LlamaForCausalLM_with_lossmask`ã€‚<br>3. åˆå§‹åŒ–å…¨æ–°çš„ LoRA å±‚ã€‚ | **åˆå§‹åŒ–çš„ PEFT æ¨¡å‹**<br>(Task: Causal LM) |
| **3. T5/BART ä»å¤´å¾®è°ƒ** | `t5-large` <br> `facebook/bart-large` | æ—¢æ—  `"adapter"`<br>ä¹Ÿæ—  `"llama"` | **é€šç”¨åŠ è½½ + åˆå§‹åŒ– LoRA**ã€‚<br>1. ä½¿ç”¨æ ‡å‡†çš„ `AutoTokenizer` å’Œ `AutoModelForSeq2SeqLM`ã€‚<br>2. åˆå§‹åŒ–å…¨æ–°çš„ LoRA å±‚ã€‚ | **åˆå§‹åŒ–çš„ PEFT æ¨¡å‹**<br>(Task: Seq2Seq) |

#### **è®­ç»ƒå™¨**
- **uie_trainer_lora.py** (1297è¡Œ):
  - è‡ªå®šä¹‰Seq2Seqè®­ç»ƒå™¨
  - å®ç°æ¢¯åº¦æ›´æ–°ã€è¯„ä¼°å›è°ƒç­‰
  - **training_stepï¼šé‡Œé¢åŒ…å«äº†è®­ç»ƒè¿‡ç¨‹ï¼ˆå‰å‘ä¼ æ’­+è®¡ç®—æŸå¤±ï¼‰**

#### **æ•°æ®å¤„ç†**
- **uie_dataset_lora.py** (519è¡Œ):
  - æ•°æ®é›†åŠ è½½å’Œé¢„å¤„ç†
  - æ”¯æŒå¤šä»»åŠ¡ã€å¤šæŒ‡ä»¤ç­–ç•¥
- **uie_collator.py** (227è¡Œ):
  - æ•°æ®æ‰¹å¤„ç†collator
  - æ”¯æŒencoder-decoderå’Œdecoder-onlyæ¨¡å‹

#### **è¯„ä¼°**
- **`compute_metrics.py`**: è®¡ç®—è¯„ä¼°æŒ‡æ ‡
- **evaluator.py** (906è¡Œ): è¯¦ç»†çš„è¯„ä¼°å™¨å®ç°(å‡†ç¡®ç‡ã€F1ç­‰)

---

### ğŸ“ **å…­ã€src/Proj_torch/** - æ ¸å¿ƒç®—æ³•å®ç° ğŸ”¥ğŸ”¥ğŸ”¥

**è¿™æ˜¯ä¿®æ”¹ç®—æ³•æ ¸å¿ƒä»£ç çš„åœ°æ–¹!**

#### **proj_projector.py** (105è¡Œ)
- **`ProjProjector`ç±»**: å®ç°æ¢¯åº¦ä½ç§©æŠ•å½±çš„æ ¸å¿ƒç®—æ³•
- **å…³é”®æ–¹æ³•**:
  - `project()`: å°†å…¨ç§©æ¢¯åº¦æŠ•å½±åˆ°ä½ç§©å­ç©ºé—´
  - `project_back()`: å°†ä½ç§©æ¢¯åº¦æŠ•å½±å›åŸç©ºé—´
  - `get_orthogonal_matrix()`: é€šè¿‡SVDåˆ†è§£è·å–æ­£äº¤çŸ©é˜µ
- **æŠ•å½±ç±»å‹**: æ”¯æŒstdã€reverse_stdã€leftã€rightã€fullç­‰å¤šç§æŠ•å½±æ–¹å¼

#### **proj_projector_tensor.py**
- **`ProjProjectorTensor`ç±»**: ç”¨äºå¤„ç†é«˜ç»´å¼ é‡çš„æŠ•å½±(>2ç»´)

#### **adamw.py** (186è¡Œ)
- **`AdamW`ä¼˜åŒ–å™¨**: è‡ªå®šä¹‰çš„AdamWå®ç°
- **æ ¸å¿ƒåŠŸèƒ½**:
  - åœ¨`step()`æ–¹æ³•ä¸­é›†æˆæ¢¯åº¦æŠ•å½±
  - åŠ¨æ€æ›´æ–°æŠ•å½±çŸ©é˜µ(æ¯`update_proj_gap`æ­¥)
  - æ”¯æŒæ¢¯åº¦æŠ•å½±å‰åçš„å¤„ç†

#### **`__init__.py`**
- å¯¼å‡º`ProjAdamW`ä¼˜åŒ–å™¨ä¾›å¤–éƒ¨ä½¿ç”¨

---

### ğŸ“ **ä¸ƒã€src/peft/** - LoRAå®ç°

ä¿®æ”¹è‡ªHuggingFace PEFTåº“:

- **lora.py** (748è¡Œ):
  - **LoRAå®ç°**: å®šä¹‰LoRAå±‚å’Œé…ç½®ã€é‡Œé¢å°±å®šä¹‰äº†loranewå‚æ•°ã€‘
  - **å…³é”®ä¿®æ”¹**: 
    - `r_sum`: è®°å½•ä¹‹å‰LoRAå‚æ•°ç»´åº¦(ç”¨äºæŒç»­å­¦ä¹ )
    - `save_loranew`: æ˜¯å¦ç‹¬ç«‹ä¿å­˜æ–°LoRAå‚æ•°
- **`peft_model.py`**: PEFTæ¨¡å‹å°è£…
- **`mapping.py`**: æ¨¡å‹æ˜ å°„
- **å…¶ä»–tuners**: adalora, prefix_tuningç­‰

### ğŸ“ **å…«ã€src/model/** - æ¨¡å‹å®šä¹‰
- **`llama.py`**: LLaMAæ¨¡å‹çš„ç‰¹æ®Šå¤„ç†

### ğŸ“ **ä¹ã€src/rouge/** - è¯„ä¼°æŒ‡æ ‡
- ROUGEåˆ†æ•°è®¡ç®—(ç”¨äºæ–‡æœ¬ç”Ÿæˆä»»åŠ¡è¯„ä¼°)


### ğŸ“ åã€scripts/ - è®­ç»ƒè„šæœ¬ï¼Œscripts_llama/ - LLaMAä¸“ç”¨è„šæœ¬
- **run.sh**: ä¸»è¿è¡Œè„šæœ¬,ä¾æ¬¡æ‰§è¡Œorder_1åˆ°order_6
- **`order_1.sh ~ order_6.sh`**: 
  - æ¯ä¸ªè„šæœ¬å®šä¹‰ä¸€ç§ä»»åŠ¡å­¦ä¹ é¡ºåº
  - åŒ…å«å¤šä¸ªä»»åŠ¡çš„è¿ç»­è®­ç»ƒ
  - é…ç½®å­¦ä¹ ç‡ã€æ‰¹å¤§å°ã€LoRAæ¨¡å—ç­‰
- **`run_llama.sh`**: LLaMAæ¨¡å‹çš„è®­ç»ƒæµç¨‹
- **`order_1/2/3.sh`**: LLaMAçš„ä¸‰ç§ä»»åŠ¡é¡ºåº

### ğŸ“ **åäºŒã€logs/ å’Œ logs_llama/** - æ—¥å¿—ç›®å½•
- å­˜å‚¨è®­ç»ƒå’Œæ¨ç†æ—¥å¿—
- `null.txt`: å ä½ç¬¦æ–‡ä»¶


## ğŸ’¡ **ç®—æ³•å·¥ä½œæµç¨‹**

1. **æ•°æ®åŠ è½½**: uie_dataset_lora.py â†’ åŠ è½½ä»»åŠ¡æ•°æ®
2. **æ¨¡å‹åˆå§‹åŒ–**: run_uie_lora.py â†’ åŠ è½½T5/LLaMA + LoRAå±‚
3. **è®­ç»ƒå¾ªç¯**: uie_trainer_lora.py â†’ æ§åˆ¶è®­ç»ƒè¿‡ç¨‹
4. **æ¢¯åº¦è®¡ç®—**: åå‘ä¼ æ’­å¾—åˆ°å…¨ç§©æ¢¯åº¦
5. **æ¢¯åº¦æŠ•å½±**: `ProjProjector.project()` â†’ é™ä½æ¢¯åº¦ç»´åº¦
6. **ä¼˜åŒ–æ›´æ–°**: `AdamW.step()` â†’ åœ¨ä½ç§©ç©ºé—´æ›´æ–°å‚æ•°
7. **æŠ•å½±å›åŸç©ºé—´**: `project_back()` â†’ åº”ç”¨åˆ°åŸæ¨¡å‹å‚æ•°
8. **æŒç»­å­¦ä¹ **: ä¿å­˜æ—§LoRAå‚æ•°,ä¸ºæ–°ä»»åŠ¡æ·»åŠ æ–°LoRAå±‚






## Question
### å‚æ•°å‘½å
ç¬¬ä¸€é˜¶æ®µ LoRA (æ—§å‚æ•°) - æ¥è‡ªä¸Šä¸€ä¸ªä»»åŠ¡
"encoder.block.0.layer.0.SelfAttention.q.lora_A.weight"       # r Ã— in_features
"encoder.block.0.layer.0.SelfAttention.q.lora_B.weight"       # out_features Ã— r

ç¬¬äºŒé˜¶æ®µ LoRA (æ–°å‚æ•°) - å½“å‰ä»»åŠ¡ ã€ä½ æ·»åŠ çš„ã€‘
"encoder.block.0.layer.0.SelfAttention.q.loranew_A.weight"    # r_new Ã— in_features
"encoder.block.0.layer.0.SelfAttention.q.loranew_B.weight"    # out_features Ã— r_new

### ææ¸…æ¥šè®­ç»ƒæ—¶è®­ç»ƒçš„æ˜¯å“ªä¸€ä¸ªå‚æ•°ï¼Ÿ
```
for name, param in model.named_parameters():
    if name.find("loranew_") != -1:
        param.requires_grad = True          # âœ… loranew_A/B å¯ä»¥è®­ç»ƒ
    elif name.find("lora_") != -1:
        param.requires_grad = False         # âŒ lora_A/B è¢«å†»ç»“
    # å…¶ä»–å‚æ•°çš„å¤„ç†...
```
> fix lora_A/B (bases of previous LoRA parameters, loaded in "load_adapter"[peft_momdel.py])
fine-tune loranew_A/B (initialized in "update_layer"[lora.py])
ä¸ºä»€ä¹ˆè¿™æ ·æï¼Ÿå·¥ç¨‹å¯èƒ½åœ¨åšæŒç»­å­¦ä¹ ï¼ˆContinual Learningï¼‰æˆ–å¤šé˜¶æ®µå¾®è°ƒ

### æƒ…å†µ1ï¸âƒ£ï¼šå½“ model_name_or_path æ˜¯**æœ¬åœ°è·¯å¾„**

```
å‚æ•°: --model_name_or_path initial_model/llama
     |
     v
æ£€æŸ¥ initial_model/llama æ˜¯å¦å­˜åœ¨ï¼Ÿ
     |
     â”œâ”€ YES âœ…
     |   â””â”€ ç›´æ¥ä»æœ¬åœ°åŠ è½½
     |       â”œâ”€ è¯»å– config.json
     |       â”œâ”€ è¯»å– pytorch_model.bin
     |       â”œâ”€ è¯»å– tokenizer.model
     |       â””â”€ è¿”å›æ¨¡å‹å¯¹è±¡
     |
     â””â”€ NO âŒ
         â””â”€ æŠ¥é”™ï¼šFileNotFoundError
             (å› ä¸ºä»£ç è®¤ä¸ºä½ ç»™çš„æ˜¯æœ¬åœ°è·¯å¾„ï¼Œä¸ä¼šå°è¯•ç½‘ç»œä¸‹è½½)
```

**å…³é”®ç‚¹**ï¼šå¦‚æœä½ æä¾›çš„æ˜¯æœ¬åœ°è·¯å¾„ï¼ŒHuggingFace åº“**ä¸ä¼šè‡ªåŠ¨ä»ç½‘ç»œä¸‹è½½**ï¼

---

### æƒ…å†µ2ï¸âƒ£ï¼šå½“ model_name_or_path æ˜¯ **HuggingFace model ID**

```
å‚æ•°: --model_name_or_path meta-llama/Llama-2-7b
     |
     v
from_pretrained() çš„åŠ è½½ä¼˜å…ˆçº§:

ä¼˜å…ˆçº§1ï¸âƒ£: æ£€æŸ¥æ˜¯å¦æ˜¯æœ¬åœ°è·¯å¾„ï¼Ÿ
    â””â”€ NO (å› ä¸ºè¿™æ˜¯ model ID æ ¼å¼)

ä¼˜å…ˆçº§2ï¸âƒ£: æ£€æŸ¥ cache_dir ç¼“å­˜ç›®å½•
    é»˜è®¤ç¼“å­˜ç›®å½•: ~/.cache/huggingface/hub/
    â”œâ”€ å­˜åœ¨å®Œæ•´æ¨¡å‹ï¼Ÿ
    |   â”œâ”€ YES âœ… ä»ç¼“å­˜åŠ è½½
    |   â””â”€ NO ç»§ç»­
    â””â”€ å­˜åœ¨ä¸å®Œæ•´ä¸‹è½½ï¼Ÿ
        â””â”€ YES ç»§ç»­æ–­ç‚¹ä¸‹è½½

ä¼˜å…ˆçº§3ï¸âƒ£: ä»ç½‘ç»œä¸‹è½½
    â”œâ”€ è¿æ¥ HuggingFace å®˜æ–¹æœåŠ¡å™¨
    â”œâ”€ ä¸‹è½½æ¨¡å‹æ–‡ä»¶åˆ°ç¼“å­˜ç›®å½•
    â”œâ”€ è§£å‹/éªŒè¯
    â””â”€ åŠ è½½åˆ°å†…å­˜
```
- **ä»£ç ä¸­æ²¡æœ‰è‡ªåŠ¨ä¸‹è½½æ¨¡å‹çš„åŠŸèƒ½** - è„šæœ¬ä¸­çš„ `from_pretrained()` æ–¹æ³•å¯ä»¥ä» HuggingFace è‡ªåŠ¨ä¸‹è½½æ¨¡å‹ï¼Œä½†éœ€è¦ç½‘ç»œè¿æ¥ä¸”é€Ÿåº¦å¯èƒ½è¾ƒæ…¢- **ä»£ç ä¸­æœ‰**ï¼š`src/run_uie_lora.py` ä½¿ç”¨ HuggingFace çš„ `from_pretrained()` æ–¹æ³•
- è¿™ä¸ªæ–¹æ³•å¯ä»¥è‡ªåŠ¨ä» HuggingFace ä¸‹è½½æ¨¡å‹ï¼ˆå¦‚æœä¼ å…¥ model ID è€Œä¸æ˜¯æœ¬åœ°è·¯å¾„ï¼‰
- **åŸä½œè€…ç¡¬ç¼–ç äº†æ¨¡å‹è·¯å¾„** - ä½¿ç”¨çš„æ˜¯ `/data/chenxu/models/t5-large` å’Œ `/data/chenxu/models/llama`ï¼Œè¿™æ˜¯ä»–çš„æœ¬åœ°è·¯å¾„   ä»¥ä¸‹è„šæœ¬æ–‡ä»¶å·²å…¨éƒ¨æ›´æ–°ï¼Œå°†æ¨¡å‹è·¯å¾„ä» `/data/chenxu/models/` æ”¹ä¸º `initial_model/`ï¼š


å½“å‰é…ç½®ï¼š
  æ¨¡å‹: LLaMA-7B (çº¦13GB fp32)
  LoRA rank: 8
  è®­ç»ƒæ–¹å¼: DeepSpeed ZeRO Stage 2 + LoRA + GaLore
  Batch size: 1 per device
  Gradient accumulation: 8
  Gradient checkpointing: å¯ç”¨
  ç²¾åº¦: bfloat16
  GPU: 2 Ã— 24GB (å¹¶è¡Œè®­ç»ƒ)
å†…å­˜æ¶ˆè€—åˆ†è§£ï¼š
  æ¨¡å‹æƒé‡ (bfloat16): ~13GB
  LoRA å‚æ•°: å¾ˆå° (rank=8, çº¦100MB)
  ä¼˜åŒ–å™¨çŠ¶æ€ (ZeRO-2åˆ†ç‰‡): ~6-8GB
  æ¢¯åº¦: ~6-7GB (ZeRO-2åˆ†ç‰‡)
  æ¿€æ´»å€¼ (å³ä½¿æœ‰gradient checkpointing): ~2-4GB
  ä¸´æ—¶ç¼“å†²åŒº: ~1-2GB
  å•å¡æ€»éœ€æ±‚: çº¦ 23-24GB

RTX 4090 æ˜¾å­˜: 24GB

ï¼æ— è®º ZeRO-3 æ˜¯å¦ offloadï¼Œåªè¦å¯ç”¨éƒ½ä¼šå‡ºç°éšè—ç»´åº¦ä¸º 0 çš„é”™è¯¯ã€‚è¿™è¯´æ˜é—®é¢˜æ ¹æœ¬ä¸åœ¨ offloadï¼Œè€Œåœ¨ ZeRO-3 ä¸ gradient_checkpointing çš„æ·±å±‚å…¼å®¹æ€§é—®é¢˜ï¼Œæˆ–è€…ä¸ PEFT/LoRA åœ¨åˆ†ç‰‡ä¸‹çš„å‚æ•°é‡æ„é—®é¢˜ã€‚
## Setup

You can install the required libraries by running 

```
pip install -r requirements.txt
```

You are also required to download the t5-large model from huggingface, put it to the folder named ```initial_model```, and rename the model folder as 't5-large'.

LLaMA2 HF is also supported. You can put your llama2 hf model to the folder named ```initial_model``` and rename the model folder as 'llama'.


## Training and Evaluation

For t5-large:

You can reproduce our experiments of order 1 to 6 by simply running ```scripts/run.sh```.

The model you have trained will be saved in ```logs_and_outputs/order_(1 to 6)/outputs_order_(1 to 6)```.

The result of each task will be saved in ```logs_and_outputs/order_(1 to 6)/outputs/TASK_NAME/predict_results.json```.

You can also check the logs during training and infering in  ```logs/order_(1 to 6).log```

For LLaMA2:

You can reproduce our experiments of order 1 to 3 by simply running ```scripts_llama/run_llama.sh```.

The model you have trained will be saved in ```logs_and_outputs_llama/order_1(2 or 3)/outputs```.

The result of each task will be saved in ```logs_and_outputs_llama/order_1(2 or 3)/outputs/TASK_NAME/predict_results.json```.

You can also check the logs during training and infering in  ```logs_llama/order_1(2 or 3)/order_1(2 or 3).log```


## Citation
```markdown
@inproceedings{wang-etal-2025-continual,
    title = "Continual Gradient Low-Rank Projection Fine-Tuning for {LLM}s",
    author = "Wang, Chenxu  and
      Lyu, Yilin  and
      Sun, Zicheng  and
      Jing, Liping",
    booktitle = "Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    year = "2025",
    publisher = "Association for Computational Linguistics",
    pages = "14815--14829",
}
```
## Acknowledgment
We acknowledge the publicly available codebase of [O-LoRA](https://github.com/cmnfriend/O-LoRA)
